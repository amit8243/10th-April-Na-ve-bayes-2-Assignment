{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc00e0-7855-4ca1-a9d1-0123e8096e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Na√Øve bayes-2 Assignment\n",
    "\"\"\"Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\"\"\"\n",
    "\n",
    "Ans: To find the probability that an employee is a smoker given that he/she uses the health insurance \n",
    "plan, we can apply Bayes theorem.\n",
    "\n",
    "Lets define the events:\n",
    "A: Employee uses the health insurance plan\n",
    "B: Employee is a smoker\n",
    "\n",
    "We are given:\n",
    "P(A) = 0.70 (Probability that an employee uses the health insurance plan)\n",
    "P(B|A) = 0.40 (Probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find:\n",
    "P(B|A) (Probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "By applying Bayes' theorem:\n",
    "\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "\n",
    "P(A|B) is the probability that an employee uses the health insurance plan given that they are a smoker.\n",
    "However, this information is not provided in the given data.\n",
    "\n",
    "Without the information about P(A|B) or additional information about the relationship between using the \n",
    "health insurance plan and being a smoker, we cannot directly calculate P(B|A). Therefore, we cannot \n",
    "determine the exact probability of an employee being a smoker given that they use the health insurance \n",
    "plan with the given information.\n",
    "\n",
    "\"\"\"Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\"\"\"\n",
    "Ans:The difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the assumptions \n",
    "they make about the underlying data and the way they handle features.\n",
    "\n",
    "##Bernoulli Naive Bayes:\n",
    "\n",
    "Assumes that features are binary (0 or 1), representing the presence or absence of a particular \n",
    "attribute.\n",
    "It is commonly used for text classification tasks where the presence or absence of words is considered.\n",
    "The feature vectors are typically represented as binary vectors, where 1 indicates the presence of a \n",
    "feature and 0 indicates its absence.\n",
    "The probability estimation involves calculating the likelihood of each feature occurring or not \n",
    "occurring in each class.\n",
    "\n",
    "##Multinomial Naive Bayes:\n",
    "\n",
    "Assumes that features follow a multinomial distribution, meaning they are discrete and can take on \n",
    "multiple values (count-based).\n",
    "It is commonly used for text classification tasks where the frequency of words or other discrete \n",
    "features is considered.\n",
    "The feature vectors are typically represented as counts of occurrences of each feature.\n",
    "The probability estimation involves calculating the likelihood of each feature occurring with different \n",
    "counts in each class.\n",
    "In summary, Bernoulli Naive Bayes is suitable for binary features, while Multinomial Naive Bayes is \n",
    "appropriate for discrete count-based features. Both algorithms assume independence among features given\n",
    "the class label and use Bayes theorem to estimate class probabilities. The choice between them depends\n",
    "on the nature of the features in your dataset, such as binary presence/absence or count-based \n",
    "representations.\n",
    "\n",
    "\"\"\"Q3. How does Bernoulli Naive Bayes handle missing values?\"\"\"\n",
    "Ans: Bernoulli Naive Bayes assumes that features are binary (0 or 1), representing the presence or \n",
    "absence of a particular attribute. When dealing with missing values in the context of Bernoulli Naive \n",
    "Bayes, there are a few possible approaches:\n",
    "\n",
    "Removing Instances: One approach is to simply remove instances that have missing values. This can be \n",
    "done if the missing values are relatively rare and removing them does not significantly impact the \n",
    "overall dataset.\n",
    "\n",
    "Imputation: Another approach is to impute the missing values with a certain value. In the case of \n",
    "Bernoulli Naive Bayes, the most common approach is to impute missing values with the most frequent \n",
    "value (0 or 1) for the corresponding feature. This assumes that the missing value is most likely to \n",
    "have the same value as the majority of instances.\n",
    "\n",
    "Treat Missing Values as a Separate Category: Instead of imputing the missing values, you can treat \n",
    "missing values as a separate category or feature level. You can assign a special value, such as -1, \n",
    "to represent missing values in the feature vector. This way, the missing value becomes a valid feature \n",
    "value in the model, and its presence (or absence) can contribute to the calculation of probabilities.\n",
    "\n",
    "The choice of how to handle missing values in Bernoulli Naive Bayes depends on the specific dataset and\n",
    "the nature of the missing data. It is important to carefully consider the implications of each approach\n",
    "and the potential impact on the accuracy and reliability of the model. Additionally, it is always \n",
    "recommended to evaluate the performance of the model with different approaches and choose the one that \n",
    "yields the best results for the given problem.\n",
    "\n",
    "\"\"\"Q4. Can Gaussian Naive Bayes be used for multi-class classification?\"\"\"\n",
    "Ans: es, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a \n",
    "type of machine learning algorithm that is based on Bayes theorem. It assumes that the features of the \n",
    "data are normally distributed and that the classes are independent of each other.\n",
    "\n",
    "To use Gaussian Naive Bayes for multi-class classification, we need to train the model on a dataset \n",
    "that has multiple classes. The model will learn the probability distribution of each feature for each \n",
    "class. When we want to classify a new data point, the model will calculate the probability that the \n",
    "data point belongs to each class. The class with the highest probability will be the predicted class.\n",
    "\n",
    "Gaussian Naive Bayes is a simple and efficient algorithm that can be used for multi-class classification.\n",
    "However, it is important to note that Gaussian Naive Bayes is not always the best choice for every \n",
    "problem. For example, if the data is not normally distributed, Gaussian Naive Bayes may not perform as \n",
    "well as other algorithms.\n",
    "\n",
    "Here are some of the advantages and disadvantages of Gaussian Naive Bayes for multi-class \n",
    "classification:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple and efficient to train and use\n",
    "Can be used for both classification and regression tasks\n",
    "Works well with continuous data\n",
    "Disadvantages:\n",
    "\n",
    "Can be sensitive to outliers\n",
    "May not perform well with data that is not normally distributed\n",
    "Requires a large amount of data to train\n",
    "Overall, Gaussian Naive Bayes is a powerful and versatile machine learning algorithm that can be used\n",
    "for multi-class classification. However, it is important to carefully consider the characteristics of \n",
    "the data before using Gaussian Naive Bayes.\n",
    "\n",
    "Here are some additional tips for using Gaussian Naive Bayes for multi-class classification:\n",
    "\n",
    "Normalize the data: Normalization is a technique that is used to scale the data so that it has a mean \n",
    "of 0 and a standard deviation of 1. This can help to improve the performance of the model.\n",
    "Use regularization: Regularization is a technique that is used to prevent the model from overfitting the\n",
    "training data. There are a number of different regularization techniques that can be used, such as L1 \n",
    "regularization and L2 regularization.\n",
    "Tune the hyperparameters: The hyperparameters of the model, such as the learning rate and the number of\n",
    "iterations, can have a significant impact on the performance of the model. It is important to tune these\n",
    "hyperparameters to get the best performance from the model.By following these tips, you can improve the\n",
    "performance of Gaussian Naive Bayes for multi-class classification.\n",
    "\n",
    "\"\"\"Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\"\"\"\n",
    "Ans:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pingouin as pg , kaggle does not support pingouin\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "spam = pd.read_csv('../input/spambase/spambase.csv')\n",
    "spam.head()\n",
    "\n",
    "x = spam[spam.drop('class', axis = 1).columns]\n",
    "y = spam['class']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y, train_size = 0.7, random_state = 42)\n",
    "\n",
    "xtrain.drop(high_corr_columns, axis = 1, inplace = True)\n",
    "xtest.drop(high_corr_columns, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(xtrain_res,ytrain_res)\n",
    "predictions = gnb.predict(xtest_res)\n",
    "accuracy = accuracy_score(ytest_res,predictions)\n",
    "f1 = f1_score(ytest_res,predictions)\n",
    "auc = roc_auc_score(ytest_res,predictions)\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('f1: ', f1)\n",
    "print('AUC: ', auc)\n",
    "\n",
    "\n",
    "#Output\n",
    "accuracy:  0.9110824742268041\n",
    "f1:  0.9076305220883534\n",
    "AUC:  0.9110824742268041\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
